{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0cTUZPEiaDgP",
        "outputId": "ed8a1b57-bbb8-4369-d7b9-3683940286dd"
      },
      "outputs": [],
      "source": [
        "from preprocessor import Preprocessor as sp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tGFattXkHleD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:10: SyntaxWarning: invalid escape sequence '\\-'\n",
            "<>:10: SyntaxWarning: invalid escape sequence '\\-'\n",
            "/var/folders/zj/vcffcfjn4tb4v987bwqkl3680000gn/T/ipykernel_4237/4141217203.py:10: SyntaxWarning: invalid escape sequence '\\-'\n",
            "  vocab = list('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ#.\\'!,\\-:;\\\"?')\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "def clean_sentence(sentence):\n",
        "  pattern = r'[^A-Za-z#.\\'!,\\-:;\\\"? ]'\n",
        "  return re.sub(pattern, '', sentence)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def one_hot_encode(text):\n",
        "    # Define the vocabulary\n",
        "    vocab = list('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ#.\\'!,\\-:;\\\"?')\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    # Create a mapping from character to index\n",
        "    char_to_index = {char: idx for idx, char in enumerate(vocab)}\n",
        "\n",
        "    # Initialize the one-hot encoded array\n",
        "    one_hot_encoded = np.zeros((len(text), vocab_size), dtype=int)\n",
        "\n",
        "    # Convert each character to one-hot encoded vector\n",
        "    for i, char in enumerate(text):\n",
        "        if char in char_to_index:  # Ensure character is in the vocabulary\n",
        "            one_hot_encoded[i, char_to_index[char]] = 1\n",
        "        else:\n",
        "            raise ValueError(f\"Character '{char}' not in vocabulary\")\n",
        "\n",
        "    return one_hot_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qdDb9pNlyIZ",
        "outputId": "986c2289-c160-4bf2-f5de-cea19cbedfd5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 1)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "num_sentences = 50_000\n",
        "file_path = '/Users/delmedigo/Dev/SpaceGen/SpaceGen/train.parquet'\n",
        "sentence_df = pd.read_parquet(file_path)\n",
        "sentence_df = sentence_df[sentence_df.sentence.apply(lambda bytes_wrong: len(bytes_wrong) <= 500 and len(bytes_wrong) >= 5)]\n",
        "sentence_df = sentence_df.sample(num_sentences)\n",
        "sentence_df.drop_duplicates(inplace=True)\n",
        "sentence_df['sentence'] = sentence_df['sentence'].apply(lambda sentence: clean_sentence(sentence))\n",
        "text_lists = sentence_df['sentence'].tolist()\n",
        "sentence_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vYBqS0LaR0gC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "data = pd.DataFrame(text_lists, columns=[\"correct_sentence\"])\n",
        "data['wrong_sentence'] = data['correct_sentence'].apply(lambda text: text.replace(' ',''))\n",
        "data['bytes_correct'] = data['correct_sentence'].apply(lambda text: sp.to_bytes_list(text))\n",
        "data['bytes_wrong'] = data['wrong_sentence'].apply(lambda text: sp.to_bytes_list(text))\n",
        "data['decision'] = data[['bytes_wrong','bytes_correct']].apply(lambda row: sp.create_decision_vector(row['bytes_wrong'], row['bytes_correct']), axis=1)\n",
        "dec_dict = {'K': 0, 'I': 1}\n",
        "data['decision'] = data['decision'].apply(lambda dec: [dec_dict[d] for d in dec])\n",
        "data = data[data.bytes_wrong.apply(lambda bytes_wrong: len(bytes_wrong) <= 500)]\n",
        "lngths = [len(bytes_wrong) for bytes_wrong in data.bytes_wrong.tolist()]\n",
        "max_len = max(lngths)\n",
        "data['bytes_wrong_padded'] = data['bytes_wrong'].apply(lambda bytes_wrong: bytes_wrong + [0]*(max_len-len(bytes_wrong)))\n",
        "data['decision_padded'] = data['decision'].apply(lambda decision: decision + [0]*(max_len-len(decision)))\n",
        "data['bytes_wrong_padded'] = data['bytes_wrong_padded'].apply(lambda bytes_wrong: np.array(bytes_wrong))\n",
        "data['decision_padded'] = data['decision_padded'].apply(lambda decision: np.array(decision))\n",
        "data['wrong_sentence_padded'] = data['wrong_sentence'].apply(lambda wrong_sentence: wrong_sentence + '#'*(max_len-len(wrong_sentence)))\n",
        "data['bytes_wrong_one_hot'] = data['wrong_sentence_padded'].apply(one_hot_encode)\n",
        "data['bytes_wrong_one_hot'] = data['bytes_wrong_one_hot'].apply(lambda bytes_wrong: np.array(bytes_wrong))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsJbnSrbUOAQ",
        "outputId": "662376c7-bee9-49df-989b-3556a847b4bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X shape: torch.Size([50000, 421, 63])\n",
            "y shape: torch.Size([50000, 421])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "from torch import device\n",
        "import torch.nn as nn\n",
        "X = np.stack(data.bytes_wrong_one_hot)\n",
        "y = np.stack(data.decision_padded)\n",
        "X = torch.tensor(X, dtype=torch.short)\n",
        "y = torch.tensor(y, dtype=torch.short)\n",
        "\n",
        "num_classes = 2\n",
        "\n",
        "#y = tf.keras.utils.to_categorical(y, num_classes=num_classes)\n",
        "\n",
        "print(f'X shape: {X.shape}')\n",
        "print(f'y shape: {y.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "dataset = TensorDataset(X, y)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "Epoch 1/10 Loss: 0.0010, Accuracy: 0.9632\n",
            "Epoch 2/10 Loss: 0.0009, Accuracy: 0.9655\n",
            "Epoch 3/10 Loss: 0.0009, Accuracy: 0.9656\n",
            "Epoch 4/10 Loss: 0.0009, Accuracy: 0.9656\n",
            "Epoch 5/10 Loss: 0.0009, Accuracy: 0.9657\n",
            "Epoch 6/10 Loss: 0.0009, Accuracy: 0.9657\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[30], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten the output for binary classification\u001b[39;00m\n\u001b[1;32m    101\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten the targets\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Focal Loss\u001b[39;00m\n\u001b[1;32m    104\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    105\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[30], line 59\u001b[0m, in \u001b[0;36mFocalLoss.forward\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Compute the focal loss\u001b[39;00m\n\u001b[1;32m     58\u001b[0m pt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(targets \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, outputs, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m outputs)\n\u001b[0;32m---> 59\u001b[0m focal_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m ((\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpt\u001b[49m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma) \u001b[38;5;241m*\u001b[39m bce_loss\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Reduce the loss based on the reduction method\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:41\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:962\u001b[0m, in \u001b[0;36mTensor.__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;129m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rsub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 962\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_VariableFunctions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "# Create Dataset and DataLoader\n",
        "dataset = TensorDataset(X, y)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.encoding = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n",
        "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.encoding = self.encoding.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        encoding = self.encoding[:, :seq_len, :].to(x.device)\n",
        "        return x + encoding\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, nhead, num_layers, dim_feedforward, max_len=500, dropout=0.1):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.fc = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.embedding(src.float())  # (batch_size, seq_len, d_model)\n",
        "        src = self.positional_encoding(src)  # (batch_size, seq_len, d_model)\n",
        "        src = self.transformer_encoder(src)  # (batch_size, seq_len, d_model)\n",
        "        output = torch.sigmoid(self.fc(src))  # (batch_size, seq_len, 1)\n",
        "        return output.squeeze(-1)  # (batch_size, seq_len)\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.04, gamma=2.0, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        # Ensure outputs and targets are float\n",
        "        outputs = outputs.float()\n",
        "        targets = targets.float()\n",
        "        \n",
        "        # Compute Binary Cross Entropy loss\n",
        "        bce_loss = nn.functional.binary_cross_entropy(outputs, targets, reduction='none')\n",
        "        \n",
        "        # Compute the focal loss\n",
        "        pt = torch.where(targets == 1, outputs, 1 - outputs)\n",
        "        focal_loss = self.alpha * ((1 - pt) ** self.gamma) * bce_loss\n",
        "        \n",
        "        # Reduce the loss based on the reduction method\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n",
        "\n",
        "# Model parameters\n",
        "input_dim = 63\n",
        "d_model = 64\n",
        "nhead = 8\n",
        "num_layers = 2\n",
        "dim_feedforward = 256\n",
        "max_len = 421\n",
        "dropout = 0.1\n",
        "\n",
        "model = TransformerEncoder(input_dim, d_model, nhead, num_layers, dim_feedforward, max_len, dropout)\n",
        "\n",
        "# Compile and Train the Model\n",
        "criterion = FocalLoss(alpha=0.04, gamma=2.0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
        "\n",
        "num_epochs = 10\n",
        "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    for i, (inputs, targets) in enumerate(dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        outputs = outputs.view(-1)  # Flatten the output for binary classification\n",
        "        targets = targets.view(-1)  # Flatten the targets\n",
        "        \n",
        "        loss = criterion(outputs, targets)  # Focal Loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        predictions = (outputs >= 0.5).long()  # Binarize predictions\n",
        "        correct_predictions += (predictions == targets).sum().item()\n",
        "        total_predictions += targets.numel()  # Total number of elements\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_accuracy = correct_predictions / total_predictions  # Accuracy\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'model.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
